mutate(profanity = profanity/n_words,
negative = negative/n_words,
positive = positive/n_words,
positivity_ratio = positive/negative,
verlan = verlan/n_words,
onomatopee = onomatopee/n_words,
je = je/n_words,
sexe = sexe/n_words,
argot = argot/n_words,
french = french/n_words)
)}
get_results = function(df,var){
df$var = unlist(df[var])
lyrics = df %>% group_by(var) %>% summarise(lyrics = paste(lyrics,collapse="\n"))
results_rappeurs = df %>% group_by(var) %>%
summarise(n_total_words = sum(n_words),
pageviews_2 = sum(pageviews_2,na.rm=T),year = mean(year,na.rm=T))
z = df %>% group_by(var) %>% measure_lexicons()
results_rappeurs = left_join(z,results_rappeurs,by="var")
results_rappeurs[c("means_word_length","n_unique_words")] = do.call(rbind, lapply(lyrics$lyrics,get_complexity))
results_rappeurs["n_unique_words_7000"] = do.call(rbind, lapply(lyrics$lyrics,get_complexity,return_mean=FALSE,n_words=7000))
results_rappeurs$diversity_residuals = lm(log(results_rappeurs$n_unique_words)~log(results_rappeurs$n_total_words))$residuals
results_rappeurs$diversity_residuals_2 = lm(log(results_rappeurs$n_unique_words)~log(results_rappeurs$n_total_words) + I(log(results_rappeurs$n_total_words)^2))$residuals
#results_with_year = rbind(results_filtered,results_year[-44])
topics_rappeurs = df %>% group_by(var) %>% summarise(
conscient = sum(topic_clean=="Rap conscient"),
egotrip = sum(topic_clean=="Egotrip/Méta-rap"),
gangsta = sum(topic_clean=="Gangsta rap"),
coquin = sum(topic_clean=="Rap coquin"),
poesie = sum(topic_clean=="Rap poétique"),
hardcore = sum(topic_clean=="Rap hardcore/Drill"),
amour = sum(topic_clean=="Chansons d'amour")
)
#Normalization
topics_rappeurs[-1] = topics_rappeurs[-1]/rowSums(topics_rappeurs[-1])
topics_rappeurs[-1] = log(topics_rappeurs[-1]/(1-topics_rappeurs[-1])+.01) #Logit transform
#Add a minimum of 1% in topic to avoid infinite
topics_rappeurs$categorie = colnames(topics_rappeurs)[1+apply(topics_rappeurs[-1],1,which.max)]
results_rappeurs_with_topics = left_join(topics_rappeurs,results_rappeurs,by="var")
results_rappeurs_with_topics$profanity_log = log(results_rappeurs_with_topics$profanity)
results_rappeurs_with_topics$verlan_log = log(results_rappeurs_with_topics$verlan)
results_rappeurs_with_topics$sexe_log = log(results_rappeurs_with_topics$sexe)
results_rappeurs_with_topics$onomatopee_sqrt = sqrt(results_rappeurs_with_topics$onomatopee)
row.names(results_rappeurs_with_topics) = unlist(results_rappeurs_with_topics$var)
return(results_rappeurs_with_topics)
}
results_rappeurs_with_topics = get_results(corpus,"artist")
library(FactoMineR)
library(factoextra)
variables_excluded = c("var","artist","birthdate_artist","age_artist","year","diversity_residuals","n_words","n_total_words","n_unique_words","n_unique_words_7000","profanity","verlan","sexe","pageviews_2","categorie","onomatopee")
df_pca = filter(results_rappeurs_with_topics,n_words > 22000)
#df_pca = df_pca[-18,]
df_pca_var = df_pca[,!colnames(df_pca) %in% variables_excluded]
pca = PCA(df_pca_var,
scale.unit = TRUE,graph = FALSE)
rownames(pca$ind$coord) = df_pca$var
coord_indiv = data.frame(pca$ind$coord[,1:2])
colnames(coord_indiv) = c("Littérarité","Hardcorité")
coord_indiv$artist = df_pca$var
coord_indiv$pageviews_2 = df_pca$pageviews_2
coord_indiv$Littérarité  = - coord_indiv$Littérarité
coord_var = data.frame(pca$var$coord[,1:2])
colnames(coord_var) = c("Littérarité","Hardcorité")
coord_var$Littérarité  = - coord_var$Littérarité
coord_var$variable = c("Rap conscient","Egotrip/Méta-rap","Gangsta rap","Rap coquin","Rap poétique","Rap hardcore/Drill","Chansons d'amour","Mots négatifs","Mots positifs","Je","Mots français","Argot","Haineux","Sexisme","Ratio positivité","Longueur des mots","Diversité lexicale","Insultes","Verlan","Mots sexuels","Onomatopées")
top_rappers = head(results_rappeurs_with_topics$var[order(-results_rappeurs_with_topics$pageviews_2)],200)
plot = ggplot(filter(coord_indiv,var %in% c(top_rappers,"Ali (FRA)")),
aes(Littérarité,Hardcorité,label=artist,size=pageviews_2)) + geom_text(family="EB Garamond") +
geom_text(data=coord_var,family="EB Garamond",aes(x=5*Littérarité,y=7*Hardcorité,label=variable,color='red',size=1500)) + scale_color_discrete(guide="none") + scale_size_continuous(range = c(1.5, 7),guide="none") + guides(fill="none")
plot = ggplot(filter(coord_indiv,artist %in% c(top_rappers,"Ali (FRA)")),
aes(Littérarité,Hardcorité,label=artist,size=pageviews_2)) + geom_text(family="EB Garamond") +
geom_text(data=coord_var,family="EB Garamond",aes(x=5*Littérarité,y=7*Hardcorité,label=variable,color='red',size=1500)) + scale_color_discrete(guide="none") + scale_size_continuous(range = c(1.5, 7),guide="none") + guides(fill="none")
morphalou
dico = read.csv("~/code/docker_gallicagram/gallicagram/morphalou.csv")
dico
which(dico$flexion=="pookie")
which(dico$flexion=="mec")
which(dico$flexion=="schneck")
which(dico$flexion=="shneck")
which(dico$flexion=="beauf")
which(dico$flexion=="beuh")
which(dico$flexion=="bog")
which(dico$flexion=="bof")
which(dico$flexion=="bougnoule")
z = which(dico$flexion=="bougnoule")
dico[z,]
bad_words = c(bad_words,"fuck","bitch","nigga")
##List made with the wiktionnaire
bad_words = readLines("https://raw.githubusercontent.com/darwiin/french-badwords-list/master/list.txt")
bad_words = c(bad_words,"fuck","bitch","nigga")
library(janitor)
library(tokenizers)
library(stringr)
library(RTextTools)
library(plotly)
library(tidyr)
library(dplyr)
insult_counts = c()
for(text in corpus$lyrics){
text = str_replace_all(text,"'|’"," ")
words = unlist(tokenize_words(tolower(text)))
counts = table(words)
z = which(names(counts) %in% bad_words)
insult_counts = c(insult_counts,sum(counts[z]))
print(i)
}
insult_counts = c()
for(text in corpus$lyrics){
text = str_replace_all(text,"'|’"," ")
words = unlist(tokenize_words(tolower(text)))
counts = table(words)
z = which(names(counts) %in% bad_words)
insult_counts = c(insult_counts,sum(counts[z]))
}
len(insult_counts)
length(insult_counts)
cor(insult_counts,corpus$n_profanity)
hist(insult_counts-corpus$n_profanity)
hist(insult_counts-corpus$n_profanity,breaks=100)
min(insult_counts-corpus$n_profanity,breaks=100)
corpus$n_profanity = insult_counts
write.csv(corpus,"~/LRFAF/corpus.csv",row.names = F)
length(positive)
positive_words = read.csv("/Users/benoit2c/code/Negativity_gallicagram/Data/pos.csv")
negative_words = read.csv("/Users/benoit2c/code/Negativity_gallicagram/Data/neg.csv")
nrow(positive_words)
nrow(positive_words)/500
sample(positive_words$x)
sample(positive_words$x,20)
sample(negative_words$x,20)
year_end = 2024
c = filter(corpus,!is.na(year),year <= year_end) %>% group_by(artist) %>%
summarise(n_words = sum(n_words),lyrics = paste(lyrics,collapse="\n"),
date_start = min(year,na.rm=T),date_end = max(year,na.rm=T)
)
c[c("mean_start","diversity_start")] = do.call(rbind, lapply(c$lyrics,get_complexity,n_words=10000))
c[c("mean_start","diversity_start")] = do.call(rbind, lapply(c$lyrics,get_complexity,n_words=10000))
c[c("mean_end","diversity_end")] = do.call(rbind, lapply(c$lyrics,get_complexity,n_words=10000,tail=TRUE))
plot_start_end = function(df,var,name){
var1 = paste(var,"start",sep="_")
var2 = paste(var,"end",sep="_")
df_reshaped = gather(df[c(var1,var2)],"Période","value")
df_reshaped$Période[df_reshaped$Période==var1] ="Début de carrière"
df_reshaped$Période[df_reshaped$Période==var2] ="Fin de carrière"
ggplot(df_reshaped,aes(value,fill=Période)) +
geom_density(alpha=.5) +
xlab(name) +  ylab("Densité")
}
diff_start_end = function(var,df, scaled= FALSE,prop=FALSE,p=FALSE){
var1 = paste(var,"start",sep="_")
var2 = paste(var,"end",sep="_")
start = unlist(df[var1])
end = unlist(df[var2])
if(p){return(t.test(start,end)$p.value %>% format.pval(digits=2,eps = .001))
}else if(prop){
return(mean(end < start))
}else if(scaled){
return((mean(end)-mean(start))/sd(c(start,end)))
}else{return(mean(end)/mean(start))}
}
diff_start_end = function(var,df, scaled= FALSE,prop=FALSE,p=FALSE){
var1 = paste(var,"start",sep="_")
var2 = paste(var,"end",sep="_")
start = unlist(df[var1])
end = unlist(df[var2])
if(p){return(t.test(start,end)$p.value %>% format.pval(digits=2,eps = .001))
}else if(prop){
return(mean(end < start))
}else if(scaled){
return((mean(end)-mean(start))/sd(c(start,end)))
}else{return(mean(end)/mean(start))}
}
c_start = filter(corpus,!is.na(year),artist %in% c$artist,year <= year_end) %>% group_by(artist) %>% slice_head(n=25)%>% measure_lexicons()
c_end = filter(corpus,!is.na(year),artist %in% c$artist,year <= year_end) %>% group_by(artist) %>%
slice_tail(n=25) %>% measure_lexicons()
c_start_end = left_join(c_start,c_end,by="artist",suffix = c("_start","_end"))
c_start_end = left_join(c_start,c_end,by="artist",suffix = c("_start","_end"))
c = left_join(c,c_start_end,by="artist")
Variable = c("diversity","mean","profanity","negative","positive","verlan","je","french","sexe","hate","sexism")
diff = data.frame(Variable)
difference = (100*(sapply(diff$Variable,FUN = diff_start_end,df=c)-1)) %>% round()
diff["Différence"] = difference %>% paste("%")
diff["Différence standardisée"] = sapply(diff$Variable,FUN = diff_start_end,df=c,scaled=TRUE) %>% round(2)
diff["Fréquence de chute"] = (100*sapply(diff$Variable,FUN = diff_start_end,df=c,prop=TRUE))%>% round() %>% paste("%")
diff["p-value"] = sapply(diff$Variable,FUN = diff_start_end,df=c,p=TRUE)
diff$Variable = c("Diversité lexicale sur 10 000 mots","Longueur moyenne des mots","Fréquences des insultes","Fréquences des mots négatifs","Fréquences des mots positifs","Fréquence du verlan",'Fréquence du "je"',"Part des mots dans le dictionnaire","Fréquence des mots sexuels","Fréquence des discours sexistes","Fréquence des discours haineux")
diff
year_end = 2014
c = filter(corpus,!is.na(year),year <= year_end) %>% group_by(artist) %>%
summarise(n_words = sum(n_words),lyrics = paste(lyrics,collapse="\n"),
date_start = min(year,na.rm=T),date_end = max(year,na.rm=T)
)
c[c("mean_start","diversity_start")] = do.call(rbind, lapply(c$lyrics,get_complexity,n_words=10000))
c[c("mean_start","diversity_start")] = do.call(rbind, lapply(c$lyrics,get_complexity,n_words=10000))
c[c("mean_end","diversity_end")] = do.call(rbind, lapply(c$lyrics,get_complexity,n_words=10000,tail=TRUE))
c_start = filter(corpus,!is.na(year),artist %in% c$artist,year <= year_end) %>% group_by(artist) %>% slice_head(n=25)%>% measure_lexicons()
c_start = filter(corpus,!is.na(year),artist %in% c$artist,year <= year_end) %>% group_by(artist) %>% slice_head(n=25)%>% measure_lexicons()
c_end = filter(corpus,!is.na(year),artist %in% c$artist,year <= year_end) %>% group_by(artist) %>%
slice_tail(n=25) %>% measure_lexicons()
c_start_end = left_join(c_start,c_end,by="artist",suffix = c("_start","_end"))
c = left_join(c,c_start_end,by="artist")
Variable = c("diversity","mean","profanity","negative","positive","verlan","je","french")
difference_before_2014 = (100*(sapply(Variable,FUN = diff_start_end,df=c)-1)) %>% round()
difference_before_2014
Variable = c("diversity","mean","profanity","negative","positive","verlan","je","french","sexe","hate","sexism")
difference_before_2014 = (100*(sapply(Variable,FUN = diff_start_end,df=c)-1)) %>% round()
difference_before_2014
# Run the application
shinyApp(ui = ui, server = server)
View(diff)
library(ggplot2)
c_year = filter(corpus,year %in% 1995:2023) %>% group_by(year) %>% measure_lexicons()
c_year_topic = filter(corpus,year %in% 1990:2023) %>% group_by(year,topic_clean) %>% measure_lexicons() %>% filter(n_words > 1000)
ggplot(c_year,aes(year,n_words)) + geom_area() + xlab("Année") + ylab("Volume du corpus (en mots)")
ggplot(data = filter(results_rappeurs_with_topics,pageviews_2>170),aes(x=hate,y=sexism,label=var,size=pageviews_2)) + geom_text(family = "EB Garamond") + scale_size_continuous(guide="none")
filter(corpus,pageviews > 10000,hate > .6)
View(filter(corpus,pageviews > 10000,hate > .6))
View(filter(corpus,pageviews > 10000,hate > .55))
z = filter(corpus,n_lines > 10)
cor.test(z$sexism,z$n_words/z$n_lines)
cor.test(z$hate,z$n_words/z$n_lines)
mean(z$hate)
mean(z$sexism)
z %>% group_by("year") %>% summarise(n = mean(n_words/n_lines))
z %>% group_by(year) %>% summarise(n = mean(n_words/n_lines))
View(z %>% group_by(year) %>% summarise(n = mean(n_words/n_lines)))
plot(z$hate,z$n_words/z$n_lines)
plot(z$n_words/z$n_lines,z$sexism)
ggplot(z,aes(n_words/n_lines,sexism)) + geom_point() + geom_smooth()
ggplot(z,aes(n_words/n_lines,sexism)) + geom_point(alpha=.1,s=.1) + geom_smooth()
ggplot(z,aes(n_words/n_lines,sexism)) + geom_point(alpha=.1,size=.1) + geom_smooth()
ggplot(corpus,aes(n_words/n_lines,sexism)) + geom_point(alpha=.1,size=.1) + geom_smooth()
cor.test(corpus$sexism,corpus$n_words/z$n_lines)
ggplot(filter(corpus,n_words/n_lines<30),aes(n_words/n_lines,sexism)) + geom_point(alpha=.1,size=.1) + geom_smooth()
ggplot(filter(z,n_words/n_lines<30),aes(n_words/n_lines,sexism)) + geom_point(alpha=.1,size=.1) + geom_smooth()
cor.test(z$sexism,z$n_words/z$n_lines)
ggplot(filter(z,n_words/n_lines<30),aes(n_words/n_lines,sexism)) + geom_point(alpha=.1,size=.1) + geom_smooth(method="lm")
ggplot(filter(corpus,n_words/n_lines<30),aes(n_words/n_lines,sexism)) + geom_point(alpha=.1,size=.1) + geom_smooth(method="lm")
corpus$sexism_residuals = lm(corpus,sexism~n_words/n_lines)$residuals
corpus$sexism_residuals = lm(data=corpus,sexism~n_words/n_lines)$residuals
which(rite.csv(corpus,"~/LRFAF/corpus.csv",row.names = F))
which(is.na(corpus$sexism))
corpus$sexism_residuals[!is.na(corpus$sexism)] = lm(data=corpus,sexism~n_words/n_lines)$residuals
measure_lexicons = function(df){
return(df %>% summarise(
birthdate_artist = mean(birthdate_artist),
age_artist = mean(age_artist),
profanity = sum(n_profanity),
negative = sum(n_negative),
positive = sum(n_positive),
verlan = sum(n_verlan),
onomatopee = sum(n_onomatopee),
je = sum(n_je),
french = sum(n_french_words),
sexe = sum(n_sexe),
argot = sum(n_argot),
hate = weighted.mean(hate,n_lines),
sexism = weighted.mean(sexism_residuals,n_lines),
n_words =sum(n_words))%>%
mutate(profanity = profanity/n_words,
negative = negative/n_words,
positive = positive/n_words,
positivity_ratio = positive/negative,
verlan = verlan/n_words,
onomatopee = onomatopee/n_words,
je = je/n_words,
sexe = sexe/n_words,
argot = argot/n_words,
french = french/n_words)
)}
get_results = function(df,var){
df$var = unlist(df[var])
lyrics = df %>% group_by(var) %>% summarise(lyrics = paste(lyrics,collapse="\n"))
results_rappeurs = df %>% group_by(var) %>%
summarise(n_total_words = sum(n_words),
pageviews_2 = sum(pageviews_2,na.rm=T),year = mean(year,na.rm=T))
z = df %>% group_by(var) %>% measure_lexicons()
results_rappeurs = left_join(z,results_rappeurs,by="var")
results_rappeurs[c("means_word_length","n_unique_words")] = do.call(rbind, lapply(lyrics$lyrics,get_complexity))
results_rappeurs["n_unique_words_7000"] = do.call(rbind, lapply(lyrics$lyrics,get_complexity,return_mean=FALSE,n_words=7000))
results_rappeurs$diversity_residuals = lm(log(results_rappeurs$n_unique_words)~log(results_rappeurs$n_total_words))$residuals
results_rappeurs$diversity_residuals_2 = lm(log(results_rappeurs$n_unique_words)~log(results_rappeurs$n_total_words) + I(log(results_rappeurs$n_total_words)^2))$residuals
#results_with_year = rbind(results_filtered,results_year[-44])
topics_rappeurs = df %>% group_by(var) %>% summarise(
conscient = sum(topic_clean=="Rap conscient"),
egotrip = sum(topic_clean=="Egotrip/Méta-rap"),
gangsta = sum(topic_clean=="Gangsta rap"),
coquin = sum(topic_clean=="Rap coquin"),
poesie = sum(topic_clean=="Rap poétique"),
hardcore = sum(topic_clean=="Rap hardcore/Drill"),
amour = sum(topic_clean=="Chansons d'amour")
)
#Normalization
topics_rappeurs[-1] = topics_rappeurs[-1]/rowSums(topics_rappeurs[-1])
topics_rappeurs[-1] = log(topics_rappeurs[-1]/(1-topics_rappeurs[-1])+.01) #Logit transform
#Add a minimum of 1% in topic to avoid infinite
topics_rappeurs$categorie = colnames(topics_rappeurs)[1+apply(topics_rappeurs[-1],1,which.max)]
results_rappeurs_with_topics = left_join(topics_rappeurs,results_rappeurs,by="var")
results_rappeurs_with_topics$profanity_log = log(results_rappeurs_with_topics$profanity)
results_rappeurs_with_topics$verlan_log = log(results_rappeurs_with_topics$verlan)
results_rappeurs_with_topics$sexe_log = log(results_rappeurs_with_topics$sexe)
results_rappeurs_with_topics$onomatopee_sqrt = sqrt(results_rappeurs_with_topics$onomatopee)
row.names(results_rappeurs_with_topics) = unlist(results_rappeurs_with_topics$var)
return(results_rappeurs_with_topics)
}
results_rappeurs_with_topics = get_results(corpus,"artist")
ggplot(data = filter(results_rappeurs_with_topics,pageviews_2>170),aes(x=hate,y=sexism,label=var,size=pageviews_2)) + geom_text(family = "EB Garamond") + scale_size_continuous(guide="none")
ggplot(filter(c_year,year>1999),aes(year,scale(hate))) + geom_line() + geom_line(aes(year,scale(sexism)),color="red")
c_year = filter(corpus,year %in% 1995:2023) %>% group_by(year) %>% measure_lexicons()
ggplot(filter(c_year,year>1999),aes(year,scale(hate))) + geom_line() + geom_line(aes(year,scale(sexism)),color="red")
ggplot(filter(c_year,year>1999),aes(year,scale(hate))) + geom_line() + geom_line(aes(year,1.1*scale(sexism)),color="red")
ggplot(filter(c_year,year>1999),aes(year,scale(hate))) + geom_line() + geom_line(aes(year,1.1*scale(sexism)),color="red")
ggplot(filter(c_year,year>1999),aes(year,scale(hate))) + geom_line() + geom_line(aes(year,scale(sexism)+.2),color="red")
ggplot(filter(c_year,year>1999),aes(year,scale(hate))) + geom_line() + geom_line(aes(year,scale(sexism)+.4),color="red")
ggplot(filter(c_year,year>1999),aes(year,scale(hate))) + geom_line() + geom_line(aes(year,scale(sexism)+.5),color="red")
measure_lexicons()
?measure_lexicons()
measure_lexicons
difference_before_2014
year_end = 2024
c = filter(corpus,!is.na(year),year <= year_end) %>% group_by(artist) %>%
summarise(n_words = sum(n_words),lyrics = paste(lyrics,collapse="\n"),
date_start = min(year,na.rm=T),date_end = max(year,na.rm=T)
)
c = filter(c,n_words > 30000,c$date_end-c$date_start > 4)
c[c("mean_start","diversity_start")] = do.call(rbind, lapply(c$lyrics,get_complexity,n_words=10000))
c[c("mean_start","diversity_start")] = do.call(rbind, lapply(c$lyrics,get_complexity,n_words=10000))
c[c("mean_end","diversity_end")] = do.call(rbind, lapply(c$lyrics,get_complexity,n_words=10000,tail=TRUE))
plot_start_end = function(df,var,name){
var1 = paste(var,"start",sep="_")
var2 = paste(var,"end",sep="_")
df_reshaped = gather(df[c(var1,var2)],"Période","value")
df_reshaped$Période[df_reshaped$Période==var1] ="Début de carrière"
df_reshaped$Période[df_reshaped$Période==var2] ="Fin de carrière"
ggplot(df_reshaped,aes(value,fill=Période)) +
geom_density(alpha=.5) +
xlab(name) +  ylab("Densité")
}
diff_start_end = function(var,df, scaled= FALSE,prop=FALSE,p=FALSE){
var1 = paste(var,"start",sep="_")
var2 = paste(var,"end",sep="_")
start = unlist(df[var1])
end = unlist(df[var2])
if(p){return(t.test(start,end)$p.value %>% format.pval(digits=2,eps = .001))
}else if(prop){
return(mean(end < start))
}else if(scaled){
return((mean(end)-mean(start))/sd(c(start,end)))
}else{return(mean(end)/mean(start))}
}
c_start = filter(corpus,!is.na(year),artist %in% c$artist,year <= year_end) %>% group_by(artist) %>% slice_head(n=25)%>% measure_lexicons()
c_end = filter(corpus,!is.na(year),artist %in% c$artist,year <= year_end) %>% group_by(artist) %>%
slice_tail(n=25) %>% measure_lexicons()
c_start_end = left_join(c_start,c_end,by="artist",suffix = c("_start","_end"))
c = left_join(c,c_start_end,by="artist")
Variable = c("diversity","mean","profanity","negative","positive","verlan","je","french","sexe","hate","sexism")
diff = data.frame(Variable)
difference = (100*(sapply(diff$Variable,FUN = diff_start_end,df=c)-1)) %>% round()
diff["Différence"] = difference %>% paste("%")
diff["Différence standardisée"] = sapply(diff$Variable,FUN = diff_start_end,df=c,scaled=TRUE) %>% round(2)
diff["Fréquence de chute"] = (100*sapply(diff$Variable,FUN = diff_start_end,df=c,prop=TRUE))%>% round() %>% paste("%")
diff["p-value"] = sapply(diff$Variable,FUN = diff_start_end,df=c,p=TRUE)
row.names(diff) = Variable
diff$Variable = c("Diversité lexicale sur 10 000 mots","Longueur moyenne des mots","Fréquences des insultes","Fréquences des mots négatifs","Fréquences des mots positifs","Fréquence du verlan",'Fréquence du "je"',"Part des mots dans le dictionnaire","Fréquence des mots sexuels","Fréquence des discours sexistes","Fréquence des discours haineux")
diff
c_start$sexism
get_complexity = function(text,n_words="none",return_mean=TRUE,tail=FALSE){
words = tokenize_words(str_replace_all(text,"'|’"," "),simplify = TRUE)
if(n_words == "none"){n_words = length(words)}
if(tail){words = tail(words, n_words)
}else{words = words[1:n_words]}
if(return_mean){
return(c(mean(nchar(words),na.rm=T),length(unique(words))))
}else{return(length(unique(words)))}
}
measure_lexicons = function(df){
return(df %>% summarise(
birthdate_artist = mean(birthdate_artist),
age_artist = mean(age_artist),
profanity = sum(n_profanity),
negative = sum(n_negative),
positive = sum(n_positive),
verlan = sum(n_verlan),
onomatopee = sum(n_onomatopee),
je = sum(n_je),
french = sum(n_french_words),
sexe = sum(n_sexe),
argot = sum(n_argot),
hate = weighted.mean(hate,n_lines),
sexism = weighted.mean(sexism,n_lines),
n_words =sum(n_words))%>%
mutate(profanity = profanity/n_words,
negative = negative/n_words,
positive = positive/n_words,
positivity_ratio = positive/negative,
verlan = verlan/n_words,
onomatopee = onomatopee/n_words,
je = je/n_words,
sexe = sexe/n_words,
argot = argot/n_words,
french = french/n_words)
)}
c = filter(corpus,!is.na(year),year <= year_end) %>% group_by(artist) %>%
summarise(n_words = sum(n_words),lyrics = paste(lyrics,collapse="\n"),
date_start = min(year,na.rm=T),date_end = max(year,na.rm=T)
)
c[c("mean_start","diversity_start")] = do.call(rbind, lapply(c$lyrics,get_complexity,n_words=10000))
c[c("mean_start","diversity_start")] = do.call(rbind, lapply(c$lyrics,get_complexity,n_words=10000))
c[c("mean_end","diversity_end")] = do.call(rbind, lapply(c$lyrics,get_complexity,n_words=10000,tail=TRUE))
plot_start_end = function(df,var,name){
var1 = paste(var,"start",sep="_")
var2 = paste(var,"end",sep="_")
df_reshaped = gather(df[c(var1,var2)],"Période","value")
df_reshaped$Période[df_reshaped$Période==var1] ="Début de carrière"
df_reshaped$Période[df_reshaped$Période==var2] ="Fin de carrière"
ggplot(df_reshaped,aes(value,fill=Période)) +
geom_density(alpha=.5) +
xlab(name) +  ylab("Densité")
}
diff_start_end = function(var,df, scaled= FALSE,prop=FALSE,p=FALSE){
var1 = paste(var,"start",sep="_")
var2 = paste(var,"end",sep="_")
start = unlist(df[var1])
end = unlist(df[var2])
if(p){return(t.test(start,end)$p.value %>% format.pval(digits=2,eps = .001))
}else if(prop){
return(mean(end < start))
}else if(scaled){
return((mean(end)-mean(start))/sd(c(start,end)))
}else{return(mean(end)/mean(start))}
}
c_start = filter(corpus,!is.na(year),artist %in% c$artist,year <= year_end) %>% group_by(artist) %>% slice_head(n=25)%>% measure_lexicons()
c_end = filter(corpus,!is.na(year),artist %in% c$artist,year <= year_end) %>% group_by(artist) %>%
slice_tail(n=25) %>% measure_lexicons()
c_start_end = left_join(c_start,c_end,by="artist",suffix = c("_start","_end"))
c = left_join(c,c_start_end,by="artist")
Variable = c("diversity","mean","profanity","negative","positive","verlan","je","french","sexe","hate","sexism")
diff = data.frame(Variable)
difference = (100*(sapply(diff$Variable,FUN = diff_start_end,df=c)-1)) %>% round()
diff["Différence"] = difference %>% paste("%")
diff["Différence standardisée"] = sapply(diff$Variable,FUN = diff_start_end,df=c,scaled=TRUE) %>% round(2)
diff["Fréquence de chute"] = (100*sapply(diff$Variable,FUN = diff_start_end,df=c,prop=TRUE))%>% round() %>% paste("%")
diff["p-value"] = sapply(diff$Variable,FUN = diff_start_end,df=c,p=TRUE)
row.names(diff) = Variable
diff$Variable = c("Diversité lexicale sur 10 000 mots","Longueur moyenne des mots","Fréquences des insultes","Fréquences des mots négatifs","Fréquences des mots positifs","Fréquence du verlan",'Fréquence du "je"',"Part des mots dans le dictionnaire","Fréquence des mots sexuels","Fréquence des discours sexistes","Fréquence des discours haineux")
diff
year_end
nrow(corpus)
year_end = 2024
c = filter(corpus,!is.na(year),year <= year_end) %>% group_by(artist) %>%
summarise(n_words = sum(n_words),lyrics = paste(lyrics,collapse="\n"),
date_start = min(year,na.rm=T),date_end = max(year,na.rm=T)
)
c = filter(c,n_words > 30000,c$date_end-c$date_start > 4)
c[c("mean_start","diversity_start")] = do.call(rbind, lapply(c$lyrics,get_complexity,n_words=10000))
c[c("mean_end","diversity_end")] = do.call(rbind, lapply(c$lyrics,get_complexity,n_words=10000,tail=TRUE))
c[c("mean_end","diversity_end")] = do.call(rbind, lapply(c$lyrics,get_complexity,n_words=10000,tail=TRUE))
plot_start_end = function(df,var,name){
var1 = paste(var,"start",sep="_")
var2 = paste(var,"end",sep="_")
df_reshaped = gather(df[c(var1,var2)],"Période","value")
df_reshaped$Période[df_reshaped$Période==var1] ="Début de carrière"
df_reshaped$Période[df_reshaped$Période==var2] ="Fin de carrière"
ggplot(df_reshaped,aes(value,fill=Période)) +
geom_density(alpha=.5) +
xlab(name) +  ylab("Densité")
}
diff_start_end = function(var,df, scaled= FALSE,prop=FALSE,p=FALSE){
var1 = paste(var,"start",sep="_")
var2 = paste(var,"end",sep="_")
start = unlist(df[var1])
end = unlist(df[var2])
if(p){return(t.test(start,end)$p.value %>% format.pval(digits=2,eps = .001))
}else if(prop){
return(mean(end < start))
}else if(scaled){
return((mean(end)-mean(start))/sd(c(start,end)))
}else{return(mean(end)/mean(start))}
}
c_start = filter(corpus,!is.na(year),artist %in% c$artist,year <= year_end) %>% group_by(artist) %>% slice_head(n=25)%>% measure_lexicons()
c_end = filter(corpus,!is.na(year),artist %in% c$artist,year <= year_end) %>% group_by(artist) %>%
slice_tail(n=25) %>% measure_lexicons()
c_start_end = left_join(c_start,c_end,by="artist",suffix = c("_start","_end"))
c = left_join(c,c_start_end,by="artist")
Variable = c("diversity","mean","profanity","negative","positive","verlan","je","french","sexe","hate","sexism")
diff = data.frame(Variable)
difference = (100*(sapply(diff$Variable,FUN = diff_start_end,df=c)-1)) %>% round()
diff["Différence"] = difference %>% paste("%")
diff["Différence standardisée"] = sapply(diff$Variable,FUN = diff_start_end,df=c,scaled=TRUE) %>% round(2)
diff["Fréquence de chute"] = (100*sapply(diff$Variable,FUN = diff_start_end,df=c,prop=TRUE))%>% round() %>% paste("%")
diff
year_end = 2014
c = filter(corpus,!is.na(year),year <= year_end) %>% group_by(artist) %>%
summarise(n_words = sum(n_words),lyrics = paste(lyrics,collapse="\n"),
date_start = min(year,na.rm=T),date_end = max(year,na.rm=T)
)
c = filter(c,n_words > 30000,c$date_end-c$date_start > 4)
c[c("mean_start","diversity_start")] = do.call(rbind, lapply(c$lyrics,get_complexity,n_words=10000))
c[c("mean_start","diversity_start")] = do.call(rbind, lapply(c$lyrics,get_complexity,n_words=10000))
c[c("mean_end","diversity_end")] = do.call(rbind, lapply(c$lyrics,get_complexity,n_words=10000,tail=TRUE))
c_start = filter(corpus,!is.na(year),artist %in% c$artist,year <= year_end) %>% group_by(artist) %>% slice_head(n=25)%>% measure_lexicons()
c_end = filter(corpus,!is.na(year),artist %in% c$artist,year <= year_end) %>% group_by(artist) %>%
slice_tail(n=25) %>% measure_lexicons()
c_start_end = left_join(c_start,c_end,by="artist",suffix = c("_start","_end"))
c = left_join(c,c_start_end,by="artist")
Variable = c("diversity","mean","profanity","negative","positive","verlan","je","french")
difference_before_2014 = (100*(sapply(Variable,FUN = diff_start_end,df=c)-1)) %>% round()
difference_before_2014
Variable = c("diversity","mean","profanity","negative","positive","verlan","je","french","sexism","hate")
difference_before_2014 = (100*(sapply(Variable,FUN = diff_start_end,df=c)-1)) %>% round()
difference_before_2014
(100*(sapply(Variable,FUN = diff_start_end,df=c)-1)) %>% round()
